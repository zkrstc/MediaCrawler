# 评论续爬功能说明

## 🎯 功能概述

**评论续爬**功能允许你在爬取评论时中途中断，下次运行时会自动跳过已爬取评论的笔记，只爬取新笔记的评论或之前未爬取评论的笔记。

---

## ✨ 核心特性

1. **智能识别** - 基于 `comments.csv` 文件识别哪些笔记的评论已经爬过
2. **自动跳过** - 已爬取评论的笔记会自动跳过
3. **独立控制** - 与 content 续爬独立，互不影响
4. **实时统计** - 显示跳过和新爬取的评论数量

---

## 🔄 工作原理

### 1. 启动时加载已爬取记录

```
程序启动
    ↓
读取 comments.csv 文件
    ↓
提取所有 note_id（去重）
    ↓
保存到内存集合中
```

**示例**：
```csv
# comments.csv
note_id,content,nickname,...
65e159e5,很棒的分享,用户A,...
65e159e5,我也想买,用户B,...    ← 同一个笔记的多条评论
65e159e6,太好看了,用户C,...
```

**提取结果**：
- 已爬取评论的笔记：`{65e159e5, 65e159e6}`

---

### 2. 爬取时智能过滤

```
获取20条笔记的详情
    ↓
保存到 content.csv
    ↓
准备爬取评论
    ↓
检查哪些笔记的评论已爬过
    ↓
跳过：65e159e5, 65e159e6（已有评论）
    ↓
只爬取：剩余18条笔记的评论
    ↓
保存到 comments.csv
    ↓
标记为已爬取
```

---

## 📊 使用场景

### 场景1：评论爬取中途中断

```bash
# 第一次运行 - 爬取了10条笔记的content，但只爬了5条的评论就中断
python main.py

# content.csv: 10条笔记 ✅
# comments.csv: 5条笔记的评论 ✅

# 第二次运行 - 自动跳过已爬取评论的5条，继续爬剩余5条的评论
python main.py

# content.csv: 10条笔记（去重，不重复）✅
# comments.csv: 10条笔记的评论 ✅
```

---

### 场景2：只补爬评论

假设你之前关闭了评论爬取，现在想补爬：

```python
# 第一次运行（关闭评论）
ENABLE_GET_COMMENTS = False
python main.py
# 结果：只有 content.csv，没有 comments.csv

# 第二次运行（开启评论）
ENABLE_GET_COMMENTS = True
python main.py
# 结果：
# - content.csv 不会重复（续爬跳过）
# - comments.csv 开始爬取所有笔记的评论
```

---

### 场景3：评论爬取失败重试

```bash
# 第一次运行 - 20条笔记，15条评论成功，5条失败
python main.py

# comments.csv: 15条笔记的评论 ✅

# 第二次运行 - 自动跳过成功的15条，重试失败的5条
python main.py

# comments.csv: 20条笔记的评论 ✅
```

---

## 💡 日志示例

### 启动时

```
[XiaoHongShuCrawler.search] Resume crawl enabled:
  - Already crawled 50 notes
  - Already crawled comments for 30 notes
```

**解释**：
- 已爬取 50 条笔记的详情
- 已爬取 30 条笔记的评论
- 还有 20 条笔记的评论待爬取

---

### 爬取时

```
[XiaoHongShuCrawler.search] ✅ Batch completed: 20/20 notes saved successfully
[XiaoHongShuCrawler.search] 💬 Skipped 5 notes with already crawled comments
[XiaoHongShuCrawler.search] 💬 Crawling comments for 15 notes
```

**解释**：
- 本批次保存了 20 条笔记
- 其中 5 条笔记的评论已经爬过，跳过
- 只爬取剩余 15 条笔记的评论

---

## 🔍 与 Content 续爬的区别

| 特性 | Content 续爬 | Comment 续爬 |
|------|-------------|-------------|
| **依据文件** | `contents.csv` | `comments.csv` |
| **跳过对象** | 已爬取的笔记 | 已爬取评论的笔记 |
| **作用阶段** | 搜索结果过滤 | 评论爬取过滤 |
| **独立性** | 独立 | 独立 |

---

## 📋 完整流程示例

### 第一次运行（中途中断）

```
搜索结果：20条笔记
    ↓
爬取笔记详情：20条 ✅
    ↓
保存 content.csv：20条 ✅
    ↓
爬取评论：开始...
    ↓
爬取了 8 条笔记的评论
    ↓
【用户按 Ctrl+C 中断】
    ↓
保存 comments.csv：8条笔记的评论 ✅
```

**结果**：
- `content.csv`: 20条笔记
- `comments.csv`: 8条笔记的评论

---

### 第二次运行（续爬）

```
启动
    ↓
加载进度：
  - content: 20条笔记
  - comments: 8条笔记
    ↓
搜索结果：20条笔记
    ↓
Content续爬过滤：跳过20条（已爬取）
    ↓
继续搜索下一页...
    ↓
搜索结果：20条新笔记
    ↓
爬取笔记详情：20条 ✅
    ↓
保存 content.csv：追加20条（总共40条）✅
    ↓
Comment续爬过滤：
  - 这20条是新笔记，评论未爬取
  - 全部需要爬取
    ↓
爬取评论：20条笔记 ✅
    ↓
保存 comments.csv：追加20条笔记的评论（总共28条笔记）✅
```

**结果**：
- `content.csv`: 40条笔记
- `comments.csv`: 28条笔记的评论（8旧 + 20新）

---

### 第三次运行（补爬之前的评论）

假设你想补爬第一次中断时未爬取的12条笔记的评论：

```
启动
    ↓
加载进度：
  - content: 40条笔记
  - comments: 28条笔记
    ↓
搜索结果：前20条笔记（包含第一次的20条）
    ↓
Content续爬过滤：跳过20条
    ↓
爬取笔记详情：0条（全部跳过）
    ↓
Comment续爬过滤：
  - 第一次的20条笔记中，8条已有评论，12条没有
  - 需要爬取12条的评论
    ↓
爬取评论：12条笔记 ✅
    ↓
保存 comments.csv：追加12条笔记的评论（总共40条笔记）✅
```

**结果**：
- `content.csv`: 40条笔记（不变）
- `comments.csv`: 40条笔记的评论（28旧 + 12补）

---

## ⚙️ 配置选项

```python
# config.py

# 启用断点续爬（同时控制content和comment续爬）
ENABLE_RESUME_CRAWL = True

# 必须使用CSV存储
SAVE_DATA_OPTION = "csv"

# 启用评论爬取（关闭则不爬评论）
ENABLE_GET_COMMENTS = True
```

---

## 🚨 注意事项

### 1. 基于当天的CSV文件

与 content 续爬一样，评论续爬也基于**当天日期**的CSV文件：

```
search_comments_2025-10-17.csv  # 10月17日
search_comments_2025-10-18.csv  # 10月18日（新文件）
```

**跨天爬取**：会生成新的CSV文件，不会自动续爬。

---

### 2. 一个笔记可能有多条评论

`comments.csv` 中，一个笔记可能有多行记录（多条评论）：

```csv
note_id,comment_id,content,...
65e159e5,comment_001,很棒,...
65e159e5,comment_002,我也想买,...
65e159e5,comment_003,太好看了,...
```

**续爬逻辑**：只要这个 `note_id` 在 `comments.csv` 中出现过，就认为已爬取。

---

### 3. 部分评论爬取失败

如果某个笔记的评论爬取失败（返回0条评论），**不会**标记为已爬取，下次会重试。

```python
# 只有成功爬取评论后才标记
if notes_to_crawl_comments:
    await self.batch_get_note_comments(notes_to_crawl_comments, ...)
    for note_id in notes_to_crawl_comments:
        self.progress_manager.mark_comment_as_crawled(note_id)
```

---

### 4. 关闭评论爬取

如果设置 `ENABLE_GET_COMMENTS = False`：

```python
# 不会爬取评论，也不会检查评论续爬
if self.progress_manager and config.ENABLE_GET_COMMENTS:
    # 评论续爬逻辑
else:
    # 跳过评论爬取
```

---

## 🔧 故障排查

### 问题1：评论重复爬取

**可能原因**：
- `ENABLE_RESUME_CRAWL = False`（未开启）
- `comments.csv` 文件被删除
- 跨天爬取（日期变化）

**解决方案**：
```python
# 检查配置
ENABLE_RESUME_CRAWL = True
SAVE_DATA_OPTION = "csv"
ENABLE_GET_COMMENTS = True

# 检查文件
ls data/xhs/csv/search_comments_*.csv
```

---

### 问题2：评论没有续爬

**可能原因**：
- 笔记是新的，评论本来就没爬过
- `comments.csv` 文件为空

**检查方法**：
```bash
# 查看日志
[XiaoHongShuCrawler.search] Resume crawl enabled:
  - Already crawled comments for 0 notes  # ← 如果是0，说明没有已爬取的评论
```

---

### 问题3：跳过了不应该跳过的评论

**可能原因**：
- `comments.csv` 中有错误的记录

**解决方案**：
```bash
# 删除 comments.csv 重新爬取
rm data/xhs/csv/search_comments_*.csv

# 或者关闭评论续爬
ENABLE_RESUME_CRAWL = False
```

---

## ✅ 最佳实践

1. **始终开启续爬**
   ```python
   ENABLE_RESUME_CRAWL = True
   ```

2. **使用CSV存储**
   ```python
   SAVE_DATA_OPTION = "csv"
   ```

3. **在同一天内完成爬取**
   - 避免跨天导致的日期变化问题

4. **定期备份CSV文件**
   ```bash
   cp data/xhs/csv/*.csv backup/
   ```

5. **监控日志输出**
   - 查看跳过和爬取的数量是否符合预期

---

## 📈 性能优势

### 节省时间和请求

```
不使用评论续爬：
- 第一次：爬取 20 条笔记 + 10 条评论（中断）
- 第二次：重新爬取 20 条评论（浪费 10 次请求）

使用评论续爬：
- 第一次：爬取 20 条笔记 + 10 条评论（中断）
- 第二次：跳过 10 条，只爬取 10 条新评论 ✅
```

---

## 🎯 总结

- ✅ **Content 续爬** - 跳过已爬取的笔记
- ✅ **Comment 续爬** - 跳过已爬取评论的笔记
- ✅ **独立控制** - 两者互不影响
- ✅ **智能过滤** - 自动识别和跳过
- ✅ **实时统计** - 显示详细进度

**现在你可以放心爬取了！即使中途中断，评论也会自动续爬！** 🎉
