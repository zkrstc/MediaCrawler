# 断点续爬功能说明

## 🎯 功能概述

**断点续爬**功能允许你在爬取过程中随时中断（Ctrl+C），下次运行时会自动跳过已爬取的内容，从中断的地方继续爬取。

---

## ✨ 核心特性

1. **自动识别已爬取内容** - 基于 `content.csv` 文件中的记录
2. **智能跳过** - 搜索结果中已爬取的笔记会自动跳过
3. **实时统计** - 显示已跳过和新爬取的数量
4. **零配置** - 默认开启，无需额外设置

---

## 🔧 配置选项

在 `config.py` 中：

```python
# 是否启用断点续爬功能（基于已保存的CSV记录跳过已爬取的内容）
ENABLE_RESUME_CRAWL = True  # True=开启，False=关闭

# 必须配合CSV存储模式使用
SAVE_DATA_OPTION = "csv"
```

---

## 📋 工作原理

### 1. 启动时加载已爬取记录

```
程序启动
    ↓
读取 content.csv 文件
    ↓
提取所有 note_id
    ↓
保存到内存集合中
```

### 2. 爬取时智能过滤

```
获取搜索结果（20条笔记）
    ↓
过滤已爬取的笔记（例如跳过5条）
    ↓
只爬取新的笔记（剩余15条）
    ↓
标记为已爬取
    ↓
继续下一页
```

---

## 💡 使用场景

### 场景1：中途暂停后继续

```bash
# 第一次运行 - 爬取了50条后按 Ctrl+C 中断
python main.py
# ✅ content.csv 已保存 50 条记录

# 第二次运行 - 自动跳过前50条，继续爬取
python main.py
# ✅ 自动识别已爬取的 50 条
# ✅ 从第 51 条开始继续爬取
```

### 场景2：分批爬取

```bash
# 设置每次爬取30条
CRAWLER_MAX_NOTES_COUNT = 30

# 第一次运行
python main.py  # 爬取 1-30

# 第二次运行
python main.py  # 自动跳过 1-30，爬取 31-60

# 第三次运行
python main.py  # 自动跳过 1-60，爬取 61-90
```

### 场景3：多关键词爬取

```bash
# 配置多个关键词
KEYWORDS = "纸尿裤推荐,婴儿用品,母婴好物"

# 第一次运行 - 爬取"纸尿裤推荐"时中断
python main.py

# 第二次运行 - 自动跳过"纸尿裤推荐"已爬取的，继续后面的关键词
python main.py
```

---

## 📊 日志示例

### 启动时

```
[XiaoHongShuCrawler.search] Resume crawl enabled, already crawled 50 notes
```

### 爬取时

```
[XiaoHongShuCrawler.search] Skipped 5 already crawled notes
[XiaoHongShuCrawler.search] Crawling 15 notes (already crawled: 50, max: 100)
```

---

## 🔍 技术实现

### 进度管理器

```python
class CrawlProgressManager:
    def __init__(self, platform: str, crawler_type: str):
        self.crawled_ids: Set[str] = set()
    
    def load_crawled_ids(self) -> Set[str]:
        """从CSV文件加载已爬取的ID"""
        # 读取 content.csv
        # 提取 note_id 列
        # 返回ID集合
    
    def is_crawled(self, item_id: str) -> bool:
        """检查是否已爬取"""
        return item_id in self.crawled_ids
    
    def mark_as_crawled(self, item_id: str):
        """标记为已爬取"""
        self.crawled_ids.add(item_id)
```

### 爬虫集成

```python
# 1. 初始化进度管理器
if config.ENABLE_RESUME_CRAWL:
    self.progress_manager = CrawlProgressManager("xhs", "search")
    self.progress_manager.load_crawled_ids()

# 2. 过滤已爬取的笔记
if self.progress_manager:
    valid_items = [
        item for item in valid_items 
        if not self.progress_manager.is_crawled(item.get("id"))
    ]

# 3. 标记新爬取的笔记
if self.progress_manager:
    for note_detail in note_details:
        self.progress_manager.mark_as_crawled(note_detail.get("note_id"))
```

---

## 📁 文件依赖

断点续爬功能依赖以下文件：

```
data/xhs/csv/search_contents_2025-10-17.csv  # 已爬取记录
```

**重要**：
- 如果删除此文件，将从头开始爬取
- 如果修改此文件，可能导致重复或遗漏

---

## ⚙️ 配置建议

### 推荐配置（稳定爬取）

```python
ENABLE_RESUME_CRAWL = True          # 开启断点续爬
SAVE_DATA_OPTION = "csv"            # 使用CSV存储
CRAWLER_MAX_NOTES_COUNT = 50        # 每次爬取50条
MAX_CONCURRENCY_NUM = 1             # 并发数为1（更稳定）
```

### 快速爬取配置

```python
ENABLE_RESUME_CRAWL = True          # 开启断点续爬
SAVE_DATA_OPTION = "csv"            # 使用CSV存储
CRAWLER_MAX_NOTES_COUNT = 100       # 每次爬取100条
MAX_CONCURRENCY_NUM = 3             # 并发数为3（更快）
```

---

## 🚨 注意事项

### 1. 仅支持CSV模式

断点续爬功能目前仅支持 `SAVE_DATA_OPTION = "csv"` 模式。

如果使用 `db` 或 `json` 模式，需要自行实现去重逻辑。

### 2. 基于当天的CSV文件

进度记录基于**当天日期**的CSV文件：
- `search_contents_2025-10-17.csv` - 10月17日的记录
- `search_contents_2025-10-18.csv` - 10月18日的记录

**跨天爬取**：如果今天爬取了一部分，明天继续爬取，会生成新的CSV文件，不会自动续爬。

**解决方案**：
- 在同一天内完成爬取
- 或手动合并不同日期的CSV文件

### 3. 关键词变化

如果修改了 `KEYWORDS` 配置：
- 新增关键词：会正常爬取
- 删除关键词：已爬取的记录仍会保留在CSV中
- 修改关键词：会重新爬取（因为是新关键词）

### 4. 内存占用

已爬取的ID保存在内存中（Set集合）：
- 100条记录 ≈ 几KB内存
- 10000条记录 ≈ 几百KB内存
- 对性能影响极小

---

## 🔄 与去重功能的关系

断点续爬和CSV去重是**互补**的两个功能：

| 功能 | 作用时机 | 作用对象 |
|------|---------|---------|
| **断点续爬** | 搜索阶段 | 跳过已爬取的笔记 |
| **CSV去重** | 保存阶段 | 避免重复写入同一笔记 |

**组合使用**：
```
搜索结果（100条）
    ↓
断点续爬过滤（跳过50条已爬取）
    ↓
爬取新笔记（50条）
    ↓
CSV去重保护（避免重复写入）
    ↓
保存到文件
```

---

## 🛠️ 故障排查

### 问题1：重复爬取相同内容

**可能原因**：
- `ENABLE_RESUME_CRAWL = False`（未开启）
- CSV文件被删除或损坏
- 跨天爬取（日期变化）

**解决方案**：
```python
# 检查配置
ENABLE_RESUME_CRAWL = True
SAVE_DATA_OPTION = "csv"

# 检查CSV文件是否存在
ls data/xhs/csv/search_contents_*.csv
```

### 问题2：跳过了不应该跳过的内容

**可能原因**：
- CSV文件中有错误的记录
- note_id 重复

**解决方案**：
```bash
# 删除CSV文件重新开始
rm data/xhs/csv/search_contents_*.csv

# 或者关闭断点续爬
ENABLE_RESUME_CRAWL = False
```

### 问题3：日志显示"already crawled 0 notes"

**可能原因**：
- 第一次运行（正常）
- CSV文件不存在（正常）
- CSV文件为空（正常）

**无需处理**：这是正常现象，表示从头开始爬取。

---

## 📈 性能优化

### 1. 减少重复请求

断点续爬可以显著减少重复请求：

```
不使用断点续爬：
- 第一次：爬取 100 条（100次请求）
- 第二次：爬取 100 条（100次请求，其中50次重复）
- 总计：200次请求

使用断点续爬：
- 第一次：爬取 100 条（100次请求）
- 第二次：跳过 100 条，爬取新的 100 条（100次请求）
- 总计：200次请求，0次重复 ✅
```

### 2. 节省时间

```
假设每条笔记爬取耗时 2 秒：

不使用断点续爬：
- 重复爬取 50 条 = 100 秒浪费

使用断点续爬：
- 自动跳过 50 条 = 节省 100 秒 ✅
```

---

## ✅ 最佳实践

1. **始终开启断点续爬**
   ```python
   ENABLE_RESUME_CRAWL = True
   ```

2. **使用CSV存储模式**
   ```python
   SAVE_DATA_OPTION = "csv"
   ```

3. **合理设置爬取数量**
   ```python
   CRAWLER_MAX_NOTES_COUNT = 50  # 不要设置太大
   ```

4. **定期备份CSV文件**
   ```bash
   cp data/xhs/csv/search_contents_*.csv backup/
   ```

5. **在同一天内完成爬取**
   - 避免跨天导致的日期变化问题

---

**提示**：断点续爬功能已默认开启，无需额外配置即可使用！
