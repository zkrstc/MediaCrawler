"""
å°çº¢ä¹¦è¯„è®ºæƒ…æ„Ÿåˆ†æå·¥å…· - æŒ‰è¯„è®ºæ¥¼å±‚ç»Ÿè®¡ç‰ˆ
åŠŸèƒ½ï¼šå°†ä¸€çº§è¯„è®ºå’Œå®ƒçš„æ‰€æœ‰äºŒçº§è¯„è®ºä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼ˆä¸€å±‚ï¼‰ï¼Œç»¼åˆåˆ¤æ–­è¿™ä¸€å±‚çš„æƒ…æ„Ÿ
ä½¿ç”¨Ollama APIè¿›è¡Œæƒ…æ„Ÿåˆ†æ

æ›´æ–°ï¼š
- æ”¯æŒå…³é”®è¯ç›¸å…³æ€§åˆ†æï¼šä»summary CSVä¸­è¯»å–source_keywordå­—æ®µ
- åˆ†ææ—¶ä¼šåˆ¤æ–­è¯„è®ºæ˜¯å¦ä¸æœç´¢å…³é”®è¯ç›¸å…³
- å¦‚æœè¯„è®ºè®¨è®ºçš„æ˜¯å…¶ä»–äº§å“/å“ç‰Œï¼ˆä¸å…³é”®è¯æ— å…³ï¼‰ï¼Œä¼šè¢«åˆ¤å®šä¸ºä¸­æ€§
- ä¾‹å¦‚ï¼šæœç´¢"é›…è¯—å…°é»›"ï¼Œä½†è¯„è®ºåªè¯´"å…°è”»å¾ˆå¥½ç”¨"ï¼Œä¼šè¢«åˆ¤å®šä¸ºä¸­æ€§
"""

import time
import json
import requests
import pandas as pd
import logging
import traceback
from collections import defaultdict
from typing import Optional, List, Dict

# ================== é…ç½®å‚æ•° ==================
# Ollama APIé…ç½®
BASE_URL = "http://192.168.2.21:11434"  # OllamaæœåŠ¡åœ°å€
MODEL_NAME = "qwen2.5:7b"  # Ollamaæ¨¡å‹åç§°
TIMEOUT = 60  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ–‡ä»¶è·¯å¾„é…ç½®
INPUT_COMMENTS_CSV = "data/xhs/csv/search_comments_2025-10-19.csv"  # è¯„è®ºCSV
INPUT_SUMMARY_CSV = "data/xhs/csv/search_summary_with_ai_comments.csv"  # æ‘˜è¦CSV
OUTPUT_SUMMARY_CSV = "data/xhs/csv/search_summary_with_ai_comments.csv"  # æ›´æ–°åçš„æ‘˜è¦CSV
OUTPUT_DETAILS_CSV = "data/xhs/csv/comments_thread_analysis.csv"  # æŒ‰æ¥¼å±‚çš„è¯¦ç»†åˆ†æ
STATS_CSV = "data/xhs/csv/comments_thread_stats.csv"  # ç»Ÿè®¡æŠ¥å‘Š

# APIè°ƒç”¨å‚æ•°
REQUEST_INTERVAL = 0.1  # APIè°ƒç”¨é—´éš”(ç§’) - Ollamaæœ¬åœ°è°ƒç”¨å¯ä»¥æ›´å¿«

# åˆ¤å®šå‚æ•°
POSITIVE_THRESHOLD = 0.5  # ç§¯æè¯„è®ºå æ¯”è¶…è¿‡æ­¤å€¼ï¼Œåˆ¤å®šè¯¥å±‚ä¸ºç§¯æ
NEGATIVE_THRESHOLD = 0.5  # æ¶ˆæè¯„è®ºå æ¯”è¶…è¿‡æ­¤å€¼ï¼Œåˆ¤å®šè¯¥å±‚ä¸ºæ¶ˆæ

# ================== åˆå§‹åŒ–è®¾ç½® ==================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("./comments_thread_analysis.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)


# ================== æ ¸å¿ƒå‡½æ•° ==================


def analyze_thread_sentiment(thread_comments, keyword='', max_retries=3):
    """
    ä½¿ç”¨Ollama APIåˆ†ææ•´å±‚æ¥¼è¯„è®ºçš„æƒ…æ„Ÿï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    ä¸€æ¬¡æ€§ä¼ å…¥æ•´å±‚æ¥¼çš„æ‰€æœ‰è¯„è®ºï¼Œè®©AIç»¼åˆåˆ¤æ–­æ•´ä½“æƒ…æ„Ÿå€¾å‘
    
    Args:
        thread_comments: æ•´å±‚æ¥¼çš„æ‰€æœ‰è¯„è®ºåˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{'level': 1, 'content': '...'}, ...]
        keyword: æœç´¢å…³é”®è¯ï¼Œç”¨äºåˆ¤æ–­è¯„è®ºæ˜¯å¦ä¸å…³é”®è¯ç›¸å…³
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        dict: æƒ…æ„Ÿåˆ†æç»“æœ
            - sentiment: æƒ…æ„Ÿææ€§ï¼ˆ0=æ¶ˆæï¼Œ1=ä¸­æ€§ï¼Œ2=ç§¯æï¼‰
            - positive_prob: æ­£é¢æ¦‚ç‡
            - negative_prob: è´Ÿé¢æ¦‚ç‡
            - confidence: ç½®ä¿¡åº¦
            - total_comments: è¯¥æ¥¼å±‚çš„è¯„è®ºæ€»æ•°
    """
    # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼Œæ ¹æ®æ˜¯å¦æœ‰å…³é”®è¯è°ƒæ•´
    if keyword:
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚ä½ å°†æ”¶åˆ°ä¸€ä¸ªè¯„è®ºæ¥¼å±‚çš„æ‰€æœ‰è¯„è®ºï¼ˆåŒ…æ‹¬ä¸€çº§è¯„è®ºå’Œæ‰€æœ‰å›å¤ï¼‰ï¼Œè¯·ç»¼åˆåˆ†ææ•´ä¸ªæ¥¼å±‚çš„æƒ…æ„Ÿå€¾å‘ã€‚

**é‡è¦ï¼šæœ¬æ¬¡åˆ†æçš„æœç´¢å…³é”®è¯æ˜¯ã€Œ{keyword}ã€**

åˆ†ææ—¶è¯·æ³¨æ„ï¼š
1. ç»¼åˆè€ƒè™‘æ‰€æœ‰è¯„è®ºçš„æƒ…æ„Ÿ
2. æƒé‡å¯ä»¥è€ƒè™‘æ¥¼å±‚å…³ç³»ï¼ˆä¸€çº§è¯„è®ºæƒé‡æ›´é«˜ï¼‰
3. æ³¨æ„è¯„è®ºä¹‹é—´çš„äº’åŠ¨å…³ç³»å’Œæƒ…æ„Ÿä¼ é€’
4. **å…³é”®ï¼šå¿…é¡»åˆ¤æ–­è¯„è®ºå†…å®¹æ˜¯å¦ä¸å…³é”®è¯ã€Œ{keyword}ã€ç›¸å…³**
   - å¦‚æœè¯„è®ºä¸»è¦è®¨è®ºçš„æ˜¯ã€Œ{keyword}ã€ï¼Œåˆ™æ ¹æ®æƒ…æ„Ÿåˆ¤æ–­ä¸ºç§¯æ/æ¶ˆæ/ä¸­æ€§
   - å¦‚æœè¯„è®ºä¸»è¦è®¨è®ºçš„æ˜¯å…¶ä»–äº§å“/å“ç‰Œ/è¯é¢˜ï¼ˆä¸ã€Œ{keyword}ã€æ— å…³ï¼‰ï¼Œåˆ™åº”åˆ¤æ–­ä¸ºä¸­æ€§
   - ä¾‹å¦‚ï¼šå…³é”®è¯æ˜¯ã€Œé›…è¯—å…°é»›ã€ï¼Œä½†è¯„è®ºåªè¯´ã€Œå…°è”»å¾ˆå¥½ç”¨ã€ï¼Œè¿™åº”è¯¥æ˜¯ä¸­æ€§ï¼Œå› ä¸ºä¸æ˜¯åœ¨è®¨è®ºé›…è¯—å…°é»›

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
  "sentiment": 2,
  "positive_prob": 0.85,
  "negative_prob": 0.05,
  "confidence": 0.90
}}

å…¶ä¸­ï¼š
- sentiment: æƒ…æ„Ÿææ€§ï¼Œ0=æ¶ˆæï¼Œ1=ä¸­æ€§ï¼Œ2=ç§¯æ
- positive_prob: æ­£é¢æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰
- negative_prob: è´Ÿé¢æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰

åˆ¤æ–­æ ‡å‡†ï¼š
- ç§¯æ(2)ï¼šæ•´ä½“è¡¨è¾¾å¯¹ã€Œ{keyword}ã€çš„æ­£é¢æƒ…ç»ªã€èµç¾ã€æ”¯æŒã€å–œæ¬¢ç­‰
- æ¶ˆæ(0)ï¼šæ•´ä½“è¡¨è¾¾å¯¹ã€Œ{keyword}ã€çš„è´Ÿé¢æƒ…ç»ªã€æ‰¹è¯„ã€æŠ±æ€¨ã€ä¸æ»¡ç­‰
- ä¸­æ€§(1)ï¼šé™ˆè¿°äº‹å®ã€è¯¢é—®é—®é¢˜ã€æ— æ˜æ˜¾æƒ…æ„Ÿå€¾å‘ï¼Œæˆ–ä¸»è¦è®¨è®ºå…¶ä»–äº§å“/è¯é¢˜

åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—è¯´æ˜ã€‚"""
    else:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æåŠ©æ‰‹ã€‚ä½ å°†æ”¶åˆ°ä¸€ä¸ªè¯„è®ºæ¥¼å±‚çš„æ‰€æœ‰è¯„è®ºï¼ˆåŒ…æ‹¬ä¸€çº§è¯„è®ºå’Œæ‰€æœ‰å›å¤ï¼‰ï¼Œè¯·ç»¼åˆåˆ†ææ•´ä¸ªæ¥¼å±‚çš„æƒ…æ„Ÿå€¾å‘ã€‚

åˆ†ææ—¶è¯·æ³¨æ„ï¼š
1. ç»¼åˆè€ƒè™‘æ‰€æœ‰è¯„è®ºçš„æƒ…æ„Ÿ
2. æƒé‡å¯ä»¥è€ƒè™‘æ¥¼å±‚å…³ç³»ï¼ˆä¸€çº§è¯„è®ºæƒé‡æ›´é«˜ï¼‰
3. æ³¨æ„è¯„è®ºä¹‹é—´çš„äº’åŠ¨å…³ç³»å’Œæƒ…æ„Ÿä¼ é€’

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
  "sentiment": 2,
  "positive_prob": 0.85,
  "negative_prob": 0.05,
  "confidence": 0.90
}

å…¶ä¸­ï¼š
- sentiment: æƒ…æ„Ÿææ€§ï¼Œ0=æ¶ˆæï¼Œ1=ä¸­æ€§ï¼Œ2=ç§¯æ
- positive_prob: æ­£é¢æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰
- negative_prob: è´Ÿé¢æ¦‚ç‡ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰
- confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰

åˆ¤æ–­æ ‡å‡†ï¼š
- ç§¯æ(2)ï¼šæ•´ä½“è¡¨è¾¾æ­£é¢æƒ…ç»ªã€èµç¾ã€æ”¯æŒã€å–œæ¬¢ç­‰
- æ¶ˆæ(0)ï¼šæ•´ä½“è¡¨è¾¾è´Ÿé¢æƒ…ç»ªã€æ‰¹è¯„ã€æŠ±æ€¨ã€ä¸æ»¡ç­‰
- ä¸­æ€§(1)ï¼šé™ˆè¿°äº‹å®ã€è¯¢é—®é—®é¢˜ã€æ— æ˜æ˜¾æƒ…æ„Ÿå€¾å‘

åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—è¯´æ˜ã€‚"""

    # æ„å»ºè¯„è®ºå†…å®¹å­—ç¬¦ä¸²
    comments_text = "ã€è¯„è®ºæ¥¼å±‚ã€‘\n"
    for i, comment in enumerate(thread_comments, 1):
        level = comment.get('level', 1)
        content = comment.get('content', '')
        indent = "  " * (level - 1)  # æ ¹æ®å±‚çº§ç¼©è¿›
        comments_text += f"{indent}[L{level}] {content}\n"
    
    for retry in range(max_retries):
        try:
            # å‘é€è¯·æ±‚åˆ°Ollama API
            response = requests.post(
                f"{BASE_URL}/api/chat",
                json={
                    "model": MODEL_NAME,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"è¯·åˆ†æè¿™ä¸ªè¯„è®ºæ¥¼å±‚çš„æ•´ä½“æƒ…æ„Ÿï¼š\n\n{comments_text}"}
                    ],
                    "stream": False,
                    "options": {
                        "temperature": 0.3
                    }
                },
                timeout=TIMEOUT
            )
            
            if response.status_code != 200:
                logging.error(f"Ollama APIé”™è¯¯: {response.status_code}")
                raise Exception(f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
            
            result_json = response.json()
            result_text = result_json.get('message', {}).get('content', '').strip()
            
            # è§£æJSONç»“æœ
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(result_text)
            
            return {
                'sentiment': result.get('sentiment', 1),
                'positive_prob': result.get('positive_prob', 0),
                'confidence': result.get('confidence', 0),
                'negative_prob': result.get('negative_prob', 0),
                'total_comments': len(thread_comments)
            }
                
        except Exception as e:
            logging.error(f"æ¥¼å±‚æƒ…æ„Ÿåˆ†æå¤±è´¥ (é‡è¯•{retry+1}/{max_retries}): æ¥¼å±‚è¯„è®ºæ•°={len(thread_comments)} | é”™è¯¯: {str(e)}")
            if retry < max_retries - 1:
                wait_time = (retry + 1) * 2
                time.sleep(wait_time)
                continue
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ä¸­æ€§
    logging.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ{max_retries}æ¬¡ï¼‰ï¼Œè¿”å›ä¸­æ€§ç»“æœã€‚")
    return {'sentiment': 1, 'positive_prob': 0, 'confidence': 0, 'negative_prob': 0, 'total_comments': len(thread_comments)}


def build_comment_tree(comments_dict, parent_id, level=1):
    """
    é€’å½’æ„å»ºè¯„è®ºæ ‘ï¼Œæ‰¾å‡ºæ‰€æœ‰ç›´æ¥å’Œé—´æ¥å…³è”çš„å­è¯„è®º
    
    Args:
        comments_dict: æ‰€æœ‰è¯„è®ºçš„å­—å…¸ {comment_id: comment_data}
        parent_id: çˆ¶è¯„è®ºID
        level: å½“å‰å±‚çº§
    
    Returns:
        list: è¯¥çˆ¶è¯„è®ºä¸‹çš„æ‰€æœ‰å­è¯„è®ºåˆ—è¡¨ï¼ˆåŒ…æ‹¬é—´æ¥å­è¯„è®ºï¼‰
    """
    result = []
    
    for comment_id, comment_data in comments_dict.items():
        if comment_data.get('parent_id') == parent_id:
            # æ‰¾åˆ°ç›´æ¥å­è¯„è®º
            comment_info = {
                'comment_id': comment_id,
                'content': comment_data.get('content', ''),
                'level': level
            }
            result.append(comment_info)
            
            # é€’å½’æŸ¥æ‰¾è¯¥è¯„è®ºçš„å­è¯„è®ºï¼ˆé—´æ¥å…³è”ï¼‰
            sub_comments = build_comment_tree(comments_dict, comment_id, level + 1)
            result.extend(sub_comments)
    
    return result


def process_comments():
    """
    å¤„ç†è¯„è®ºCSVæ–‡ä»¶ï¼ŒæŒ‰æ¥¼å±‚è¿›è¡Œæƒ…æ„Ÿåˆ†æå¹¶ç»Ÿè®¡
    """
    # 1. è¯»å–è¯„è®ºCSV
    try:
        df_comments = pd.read_csv(INPUT_COMMENTS_CSV, encoding='utf-8-sig')
        logging.info(f"æˆåŠŸè¯»å– {len(df_comments)} æ¡è¯„è®º")
    except Exception as e:
        logging.error(f"è¯»å–è¯„è®ºCSVå¤±è´¥: {e}")
        return
    
    if df_comments.empty:
        logging.warning("è¯„è®ºCSVæ–‡ä»¶ä¸ºç©º")
        return
    
    # 2. è¯»å–æ‘˜è¦CSV
    try:
        df_summary = pd.read_csv(INPUT_SUMMARY_CSV, encoding='utf-8-sig')
        logging.info(f"æˆåŠŸè¯»å– {len(df_summary)} æ¡ç¬”è®°æ‘˜è¦")
    except Exception as e:
        logging.error(f"è¯»å–æ‘˜è¦CSVå¤±è´¥: {e}")
        return
    
    # 3. æ£€æŸ¥OllamaæœåŠ¡
    try:
        test_response = requests.get(f"{BASE_URL}/api/tags", timeout=5)
        if test_response.status_code == 200:
            logging.info("OllamaæœåŠ¡è¿æ¥æˆåŠŸ")
        else:
            logging.warning(f"OllamaæœåŠ¡å“åº”å¼‚å¸¸: {test_response.status_code}")
    except Exception as e:
        logging.error(f"OllamaæœåŠ¡è¿æ¥å¤±è´¥: {e}")
        logging.error("è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨ï¼Œå¹¶ä¸”BASE_URLé…ç½®æ­£ç¡®")
        return
    
    # 4. ä¸º summary æ–°å¢ç»Ÿè®¡åˆ—ï¼ˆåªæ·»åŠ P10å’ŒN10ï¼‰
    df_summary['P10'] = 0  # å‰10å±‚ä¸­ç§¯æçš„å±‚æ•°
    df_summary['N10'] = 0  # å‰10å±‚ä¸­æ¶ˆæçš„å±‚æ•°
    
    # 4.5 æ„å»ºnote_idåˆ°keywordçš„æ˜ å°„
    note_keyword_map = {}  # note_id -> keyword
    for idx, row in df_summary.iterrows():
        note_url = str(row.get('note_url', ''))
        keyword = str(row.get('source_keyword', '')).strip()
        
        # ä» note_url ä¸­æå– note_id
        if 'explore/' in note_url:
            try:
                note_id = note_url.split('explore/')[1].split('?')[0]
                if keyword and keyword != 'nan':
                    note_keyword_map[note_id] = keyword
                    logging.info(f"ç¬”è®° {note_id[:8]}.. å…³é”®è¯: {keyword}")
            except:
                pass
    
    logging.info(f"æˆåŠŸæ˜ å°„ {len(note_keyword_map)} ä¸ªç¬”è®°çš„å…³é”®è¯")
    
    # 5. æŒ‰ç¬”è®°IDåˆ†ç»„å¤„ç†
    note_stats = {}  # å­˜å‚¨æ¯ä¸ªç¬”è®°çš„ç»Ÿè®¡æ•°æ®
    thread_details = []  # å­˜å‚¨æ¯ä¸ªæ¥¼å±‚çš„è¯¦ç»†ä¿¡æ¯
    
    # 6. æŒ‰ç¬”è®°åˆ†ç»„è¯„è®ºï¼Œæ„å»ºè¯„è®ºå­—å…¸
    logging.info("å¼€å§‹æŒ‰ç¬”è®°åˆ†ç»„è¯„è®ºå¹¶æ„å»ºè¯„è®ºæ ‘...")
    note_comments_dict = defaultdict(dict)  # note_id -> {comment_id: comment_data}
    note_root_comments = defaultdict(list)  # note_id -> [root_comment_ids]
    
    for index, row in df_comments.iterrows():
        note_id = str(row.get('note_id', ''))
        parent_id = str(row.get('parent_comment_id', '0'))
        comment_id = str(row.get('comment_id', ''))
        content = str(row.get('content', ''))
        
        # å¤„ç†ç©ºå€¼å’ŒNaN
        if parent_id == '' or parent_id == 'nan' or pd.isna(parent_id):
            parent_id = '0'
        
        # å­˜å‚¨è¯„è®ºæ•°æ®
        note_comments_dict[note_id][comment_id] = {
            'parent_id': parent_id,
            'content': content,
            'row': row
        }
        
        # è®°å½•ä¸€çº§è¯„è®ºï¼ˆæ ¹è¯„è®ºï¼‰
        if parent_id == '0':
            note_root_comments[note_id].append(comment_id)
    
    # 7. é€ç¬”è®°ã€é€æ¥¼å±‚åˆ†ææƒ…æ„Ÿ
    logging.info("å¼€å§‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
    
    for note_id in note_root_comments.keys():
        # è·å–è¯¥ç¬”è®°çš„å…³é”®è¯
        keyword = note_keyword_map.get(note_id, '')
        keyword_info = f" [å…³é”®è¯: {keyword}]" if keyword else " [æ— å…³é”®è¯]"
        logging.info(f"\nå¤„ç†ç¬”è®°: {note_id[:15]}...{keyword_info}")
        
        if note_id not in note_stats:
            note_stats[note_id] = {
                'total_threads': 0,
                'positive_threads': 0,
                'negative_threads': 0,
                'neutral_threads': 0,
                'p10': 0,
                'n10': 0,
                'threads': []
            }
        
        thread_count = 0
        root_comments = note_root_comments[note_id]
        
        for root_comment_id in root_comments:
            thread_count += 1
            
            # è·å–ä¸€çº§è¯„è®ºå†…å®¹
            root_comment_data = note_comments_dict[note_id][root_comment_id]
            parent_content = root_comment_data['content']
            
            # æ„å»ºæ•´å±‚æ¥¼çš„è¯„è®ºåˆ—è¡¨ï¼ˆåŒ…å«ä¸€çº§è¯„è®ºå’Œæ‰€æœ‰ç›´æ¥ã€é—´æ¥å­è¯„è®ºï¼‰
            thread_comments = [{
                'comment_id': root_comment_id,
                'content': parent_content,
                'level': 1
            }]
            
            # é€’å½’è·å–æ‰€æœ‰å­è¯„è®ºï¼ˆåŒ…æ‹¬é—´æ¥å…³è”ï¼‰
            all_sub_comments = build_comment_tree(note_comments_dict[note_id], root_comment_id, level=2)
            thread_comments.extend(all_sub_comments)
            
            logging.info(f"  æ¥¼å±‚ {thread_count}: å…± {len(thread_comments)} æ¡è¯„è®ºï¼ˆåŒ…å«æ‰€æœ‰å±‚çº§ï¼‰")
            logging.info(f"    ä¸€çº§è¯„è®º: {parent_content[:50]}...")
            
            # ä¸€æ¬¡æ€§åˆ†ææ•´å±‚æ¥¼çš„æƒ…æ„Ÿï¼ˆä¼ å…¥å…³é”®è¯ï¼‰
            thread_result = analyze_thread_sentiment(thread_comments, keyword=keyword)
            time.sleep(REQUEST_INTERVAL)
            
            # è½¬æ¢sentimentæ•°å€¼ä¸ºå­—ç¬¦ä¸²
            sentiment_value = thread_result['sentiment']
            if sentiment_value == 2:
                thread_sentiment = 'positive'
            elif sentiment_value == 0:
                thread_sentiment = 'negative'
            else:
                thread_sentiment = 'neutral'
            
            logging.info(f"  æ¥¼å±‚ {thread_count} ç»¼åˆåˆ¤å®š: {thread_sentiment.upper()} "
                        f"(ç½®ä¿¡åº¦:{thread_result['confidence']:.2f}, "
                        f"æ­£é¢æ¦‚ç‡:{thread_result['positive_prob']:.2f}, "
                        f"è´Ÿé¢æ¦‚ç‡:{thread_result['negative_prob']:.2f})")
            
            # è®°å½•æ¥¼å±‚è¯¦æƒ…
            thread_details.append({
                'note_id': note_id,
                'thread_num': thread_count,
                'parent_content': parent_content[:100],
                'total_comments': len(thread_comments),
                'max_level': max(c['level'] for c in thread_comments),  # æœ€æ·±å±‚çº§
                'thread_sentiment': thread_sentiment,
                'confidence': thread_result['confidence'],
                'positive_prob': thread_result['positive_prob'],
                'negative_prob': thread_result['negative_prob']
            })
            
            # æ›´æ–°ç»Ÿè®¡
            note_stats[note_id]['total_threads'] += 1
            note_stats[note_id]['threads'].append(thread_sentiment)
            
            if thread_sentiment == 'positive':
                note_stats[note_id]['positive_threads'] += 1
            elif thread_sentiment == 'negative':
                note_stats[note_id]['negative_threads'] += 1
            else:
                note_stats[note_id]['neutral_threads'] += 1
            
            # ç»Ÿè®¡å‰10å±‚
            if thread_count <= 10:
                if thread_sentiment == 'positive':
                    note_stats[note_id]['p10'] += 1
                elif thread_sentiment == 'negative':
                    note_stats[note_id]['n10'] += 1
    
    # 8. ä¿å­˜æ¥¼å±‚è¯¦ç»†åˆ†æç»“æœ
    try:
        df_details = pd.DataFrame(thread_details)
        df_details.to_csv(OUTPUT_DETAILS_CSV, index=False, encoding='utf-8-sig')
        logging.info(f"âœ“ æ¥¼å±‚è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜: {OUTPUT_DETAILS_CSV}")
    except Exception as e:
        logging.error(f"ä¿å­˜è¯¦ç»†ç»“æœå¤±è´¥: {e}")
    
    # 9. æ›´æ–° summary CSV
    logging.info("æ›´æ–°æ‘˜è¦CSVä¸­çš„ç»Ÿè®¡æ•°æ®...")
    
    for idx, row in df_summary.iterrows():
        note_url = str(row.get('note_url', ''))
        
        # ä» note_url ä¸­æå– note_id
        if 'explore/' in note_url:
            try:
                note_id = note_url.split('explore/')[1].split('?')[0]
            except:
                note_id = ''
        else:
            note_id = ''
        
        # å¦‚æœè¯¥ç¬”è®°æœ‰ç»Ÿè®¡æ•°æ®ï¼Œæ›´æ–°åˆ° summaryï¼ˆåªæ›´æ–°P10å’ŒN10ï¼‰
        if note_id in note_stats:
            stats = note_stats[note_id]
            
            df_summary.at[idx, 'P10'] = stats['p10']
            df_summary.at[idx, 'N10'] = stats['n10']
            
            logging.info(f"ç¬”è®° {note_id[:8]}.. ç»Ÿè®¡: P10={stats['p10']}, N10={stats['n10']}")
    
    # 10. ä¿å­˜æ›´æ–°åçš„ summary CSV
    try:
        df_summary.to_csv(OUTPUT_SUMMARY_CSV, index=False, encoding='utf-8-sig')
        logging.info(f"âœ“ æ‘˜è¦CSVå·²æ›´æ–°å¹¶ä¿å­˜: {OUTPUT_SUMMARY_CSV}")
    except Exception as e:
        logging.error(f"ä¿å­˜æ‘˜è¦CSVå¤±è´¥: {e}")
    
    # 11. ç”Ÿæˆæ€»ä½“ç»Ÿè®¡æŠ¥å‘Š
    try:
        stats_data = []
        
        total_notes = len(note_stats)
        total_threads = sum(s['total_threads'] for s in note_stats.values())
        total_positive = sum(s['positive_threads'] for s in note_stats.values())
        total_negative = sum(s['negative_threads'] for s in note_stats.values())
        total_neutral = sum(s['neutral_threads'] for s in note_stats.values())
        total_p10 = sum(s['p10'] for s in note_stats.values())
        total_n10 = sum(s['n10'] for s in note_stats.values())
        
        stats_data.append({'ç»Ÿè®¡é¡¹': 'åˆ†æç¬”è®°æ€»æ•°', 'æ•°å€¼': total_notes})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'åˆ†ææ¥¼å±‚æ€»æ•°', 'æ•°å€¼': total_threads})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'ç§¯ææ¥¼å±‚æ€»æ•°', 'æ•°å€¼': total_positive})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'æ¶ˆææ¥¼å±‚æ€»æ•°', 'æ•°å€¼': total_negative})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'ä¸­æ€§æ¥¼å±‚æ€»æ•°', 'æ•°å€¼': total_neutral})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'å‰10å±‚ä¸­ç§¯ææ¥¼å±‚æ•°(P10)', 'æ•°å€¼': total_p10})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'å‰10å±‚ä¸­æ¶ˆææ¥¼å±‚æ•°(N10)', 'æ•°å€¼': total_n10})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'ç§¯ææ¯”ä¾‹(%)', 'æ•°å€¼': f'{(total_positive/total_threads*100):.2f}' if total_threads > 0 else '0.00'})
        stats_data.append({'ç»Ÿè®¡é¡¹': 'æ¶ˆææ¯”ä¾‹(%)', 'æ•°å€¼': f'{(total_negative/total_threads*100):.2f}' if total_threads > 0 else '0.00'})
        
        stats_df = pd.DataFrame(stats_data)
        stats_df.to_csv(STATS_CSV, index=False, encoding='utf-8-sig')
        logging.info(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {STATS_CSV}")
        
        # åœ¨æ§åˆ¶å°æ‰“å°ç»Ÿè®¡ç»“æœ
        print("\n" + "="*80)
        print("ğŸ“Š è¯„è®ºæ¥¼å±‚æƒ…æ„Ÿåˆ†ææ€»ä½“ç»Ÿè®¡")
        print("="*80)
        print(stats_df.to_string(index=False))
        print("="*80)
        print(f"\nğŸ’¡ è¯´æ˜ï¼š")
        print(f"  - æ¥¼å±‚ = ä¸€çº§è¯„è®º + å®ƒçš„æ‰€æœ‰äºŒçº§è¯„è®º")
        print(f"  - P10 = å‰10å±‚ä¸­ï¼Œç§¯ææ¥¼å±‚çš„æ•°é‡")
        print(f"  - N10 = å‰10å±‚ä¸­ï¼Œæ¶ˆææ¥¼å±‚çš„æ•°é‡")
        print(f"  - åˆ¤å®šæ ‡å‡†ï¼šæ¥¼å±‚ä¸­ç§¯æè¯„è®ºå æ¯”>{POSITIVE_THRESHOLD*100}%ä¸ºç§¯æï¼Œæ¶ˆæ>{NEGATIVE_THRESHOLD*100}%ä¸ºæ¶ˆæ")
        print("="*80)
        print(f"\nâœ“ å·²å°†P10ã€N10ç­‰ç»Ÿè®¡æ•°æ®æ·»åŠ åˆ°: {OUTPUT_SUMMARY_CSV}")
        print("="*80)
        
    except Exception as e:
        logging.error(f"ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {e}")
    
    return note_stats


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("å°çº¢ä¹¦è¯„è®ºæƒ…æ„Ÿåˆ†æå·¥å…· - æŒ‰æ¥¼å±‚ç»Ÿè®¡ç‰ˆ (Ollama API)")
    print("="*80)
    print(f"ğŸ“Œ åˆ†æé€»è¾‘ï¼š")
    print(f"  1. å°†ä¸€çº§è¯„è®ºå’Œå®ƒçš„æ‰€æœ‰äºŒçº§è¯„è®ºä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼ˆä¸€å±‚/ä¸€æ¥¼ï¼‰")
    print(f"  2. ç»¼åˆåˆ¤æ–­è¿™ä¸€å±‚çš„æƒ…æ„Ÿï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰")
    print(f"  3. ç»Ÿè®¡å‰10å±‚ä¸­æœ‰å¤šå°‘ç§¯æã€å¤šå°‘æ¶ˆæ")
    print("="*80)
    print(f"âš™ï¸  APIé…ç½®ï¼š")
    print(f"  - ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    print(f"  - Ollamaåœ°å€: {BASE_URL}")
    print(f"  - è°ƒç”¨é—´éš”: {REQUEST_INTERVAL} ç§’/æ¬¡")
    print("="*80)
    print(f"ğŸ“ æ–‡ä»¶è·¯å¾„ï¼š")
    print(f"  è¾“å…¥è¯„è®º: {INPUT_COMMENTS_CSV}")
    print(f"  è¾“å…¥æ‘˜è¦: {INPUT_SUMMARY_CSV}")
    print(f"  è¾“å‡ºæ‘˜è¦: {OUTPUT_SUMMARY_CSV} (ä¼šæ·»åŠ P10/N10)")
    print(f"  è¯¦ç»†åˆ†æ: {OUTPUT_DETAILS_CSV}")
    print(f"  ç»Ÿè®¡æŠ¥å‘Š: {STATS_CSV}")
    print("="*80)
    print(f"ğŸ’¡ åˆ†æè¯´æ˜ï¼š")
    print(f"  - æ¯ä¸ªæ¥¼å±‚çš„æ‰€æœ‰è¯„è®ºï¼ˆåŒ…æ‹¬é—´æ¥å›å¤ï¼‰ä¼šä¸€æ¬¡æ€§ä¼ ç»™AIåˆ†æ")
    print(f"  - AIä¼šç»¼åˆåˆ¤æ–­æ•´å±‚æ¥¼çš„æƒ…æ„Ÿå€¾å‘")
    print(f"  - å¦‚éœ€è°ƒæ•´APIè°ƒç”¨é—´éš”ï¼Œå¯ä¿®æ”¹ç¬¬27è¡Œ REQUEST_INTERVAL")
    print("="*80)
    
    # ç¡®è®¤å¼€å§‹
    confirm = input("\næŒ‰Enteré”®å¼€å§‹åˆ†æï¼Œæˆ–è¾“å…¥'q'é€€å‡º: ")
    if confirm.lower() == 'q':
        print("å·²å–æ¶ˆæ“ä½œ")
        return
    
    # å¼€å§‹åˆ†æ
    start_time = time.time()
    
    try:
        stats = process_comments()
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "="*80)
        print(f"âœ“ åˆ†æå®Œæˆï¼è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"âœ“ æ‘˜è¦CSV(å·²æ·»åŠ P10/N10): {OUTPUT_SUMMARY_CSV}")
        print(f"âœ“ æ¥¼å±‚è¯¦ç»†åˆ†æ: {OUTPUT_DETAILS_CSV}")
        print(f"âœ“ ç»Ÿè®¡æŠ¥å‘Š: {STATS_CSV}")
        print("="*80)
        
    except Exception as e:
        logging.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        logging.error(traceback.format_exc())


if __name__ == "__main__":
    main()

